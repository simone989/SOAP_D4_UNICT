import os
import json
import time
import argparse
from openai import OpenAI
from dotenv import load_dotenv

def prepare_messages(article):
    """
    Prepare messages for the batch API request.
    Currently, this is a placeholder where you can later define the exact instructions.
    For now, it returns a system message with placeholder instructions and a user message that includes
    the article_id and its threat data.
    """
    instructions_v1 = """
    You are an expert in cybersecurity, legal regulations, and the protection of the state and its citizens. Your task is to analyze the threat information provided for each asset and consolidate similar threats.

    For each asset indicated by the user, please follow these steps:
    1. Review the list of threats associated with that asset.
    2. Identify threats that are similar in nature or have overlapping explanations.
    3. Merge these similar threats into a single, concise entry that captures the essential risk and explanation.
    4. Only merge threats that refer to the same asset. Do not combine threats from different assets.
    5. Provide a final, consolidated list of threats for each asset in JSON format, preserving the threat name and a merged explanation.

    Example output format:
    {
    "assets":[
        {
            "asset":"DATA",
            "threats":[
                {
                "threat":"Data Breach",
                "explanation":"High-risk AI systems handling sensitive data may lead to unauthorized access and breaches."
                },
                {
                "threat":"Data Manipulation",
                "explanation":"Improper data handling may result in manipulation, leading to incorrect outputs."
                }
            ]
        }
    ]
    }


    Your goal is to reduce redundancy while ensuring that all essential risk information is preserved for each asset.
    """

    instructions_v2 = """
    You are an expert in threat elicitation, spanning multiple domains including cybersecurity, legal regulations, operational risks, and more. Your task is to analyze the threat information provided for each asset and consolidate similar threats. Note that the threats may originate from various contexts, not solely cybersecurity.

    For each asset indicated by the user, please follow these steps:
    1. Review the list of threats associated with that asset.
    2. Identify threats that are similar in nature or have overlapping explanations.
    3. Merge these similar threats into a single, concise entry that captures the essential risk and explanation.
    4. Only merge threats that refer to the same asset. Do not combine threats from different assets.
    5. Provide a final, consolidated list of threats for each asset in JSON format, preserving only the threat name and a merged explanation.

    Example output format (compact, with minimal whitespace):
    {"assets":[{"asset":"DATA","threats":[{"threat":"Data Breach","explanation":"High-risk systems handling sensitive data may lead to unauthorized access and breaches."},{"threat":"Data Manipulation","explanation":"Improper data handling may result in manipulation, leading to incorrect outputs."}]}]}

    Your goal is to reduce redundancy while ensuring that all essential risk information is preserved for each asset. Remember, you are performing threat elicitationâ€”that is, identifying all potential threats and vulnerabilities to assess risks and plan effective countermeasures.
    """


    # Convert the threat_regulation_list to a JSON-formatted string.
    threat_data = json.dumps(article.get("threat_regulation_list", []), indent=2)
    return [
        {"role": "assistant", "content": instructions_v2},
        {"role": "user", "content": f"Article ID: {article.get('article_id')}\nThreat Data:\n{threat_data}"}
    ]

def create_batch_entries(aggregated_articles, num_executions=1):
    """
    For each article in the aggregated_articles file, create one or more batch entries.
    Each entry has a custom_id (including the article_id and run number) and a request body
    with a model and messages generated by prepare_messages().
    """
    batch_entries = []
    for article in aggregated_articles.get("articles", []):
        article_id = article.get("article_id")
        if not article_id:
            continue
        for exec_num in range(1, num_executions + 1):
            custom_id = f"batch-{article_id}-exec-{exec_num}-{int(time.time())}"
            entry = {
                "custom_id": custom_id,
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": "o1-mini",  # Adjust the model name as needed.
                    "messages": prepare_messages(article)
                }
            }
            batch_entries.append(entry)
    return batch_entries

def write_jsonl_file(entries, output_filename):
    """
    Writes a list of entries to a JSONL file (one JSON object per line).
    """
    with open(output_filename, "w", encoding="utf-8") as f:
        for entry in entries:
            json_line = json.dumps(entry, ensure_ascii=False)
            f.write(json_line + "\n")
    print(f"Batch entries written to {output_filename}")

def invoke_openai_batch(batch_filename, client):
    """
    Uploads the batch JSONL file to OpenAI and creates a batch job.
    This is a placeholder implementation using the client's batch functionality.
    """
    with open(batch_filename, "rb") as f:
        batch_input_file = client.files.create(
            file=f,
            purpose="batch"
        )
    batch_input_file_id = batch_input_file.id
    print(f"Batch file uploaded with ID: {batch_input_file_id}")

    batch_response = client.batches.create(
        input_file_id=batch_input_file_id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
        metadata={
            "description": f"Batch processing for aggregated articles from file {batch_filename}"
        }
    )
    batch_id = batch_response.id
    print(f"Batch created with ID: {batch_id}")
    return batch_id

def main():
    batch_output_file = "batch_request.jsonl"
    parser = argparse.ArgumentParser(
        description="Generate batch requests for each article from an aggregated articles JSON file."
    )
    parser.add_argument("input_file", help="Input aggregated articles JSON file.")
    args = parser.parse_args()

    # Load the aggregated articles JSON file.
    with open(args.input_file, "r", encoding="utf-8") as infile:
        aggregated_articles = json.load(infile)

    # Create batch entries from the aggregated articles.
    batch_entries = create_batch_entries(aggregated_articles, num_executions=1)
    print(batch_entries)
    # Write the batch entries to a JSONL file.
    write_jsonl_file(batch_entries, batch_output_file)


    load_dotenv()  # Ensure environment variables are loaded.
    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    invoke_openai_batch(batch_output_file, client)

if __name__ == "__main__":
    main()
